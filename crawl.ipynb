{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hamerin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hamerin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hamerin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hamerin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import copy\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Final\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 긍정적, 부정적 단어 데이터셋 불러오기\n",
    "positiveWords = pd.Series([*map(stemmer.stem, pd.read_csv('pre_collected/positive.txt')['words'])]).unique()\n",
    "negativeWords = pd.Series([*map(stemmer.stem, pd.read_csv('pre_collected/negative.txt')['words'])]).unique()\n",
    "\n",
    "API_KEY: Final[str] = 'AIzaSyD7ZrbvvOYHD0KTu3yP-JUg_uKAMvoClNQ'  # API 요청을 위한 키\n",
    "BASE_URL: Final[str] = 'https://www.googleapis.com/youtube/v3/'  # url prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api 엔드포인트에 요청을 보내는 헬퍼 함수\n",
    "def retrieve_api(url: str, params={}, data={}, headers={}, method='GET'):\n",
    "    params = copy.deepcopy(params)\n",
    "    params.update({\n",
    "        'key': API_KEY\n",
    "    })\n",
    "\n",
    "    return requests.request(method, urllib.parse.urljoin(BASE_URL, url), params=params, data=data, headers=headers)\n",
    "\n",
    "\n",
    "# 비디오 ID를 받아, 비디오의 통계(조회수, 좋아요 수 등)을 반환한다.\n",
    "def retrieve_statistics(videoId: str) -> List[Dict[str, int]]:\n",
    "    resp = retrieve_api('videos', params={\n",
    "        'id': videoId,\n",
    "        'part': 'statistics'\n",
    "    })\n",
    "    assert(resp.ok)\n",
    "\n",
    "    return json.loads(resp.text)['items'][0]['statistics']\n",
    "\n",
    "\n",
    "# 비디오 ID를 받아, 모든 댓글을 반환한다.\n",
    "def retrieve_comments(videoId: str) -> List[List[str]]:\n",
    "    def extract_commentThread_text(item):\n",
    "        return item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "    def extract_commentThread_timestamp(item):\n",
    "        return item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "    def extract_commentThread_likecount(item):\n",
    "        return item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "\n",
    "    pageToken = None\n",
    "    result = []\n",
    "    timestamp = []\n",
    "    likecount = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            resp = retrieve_api('commentThreads', params={\n",
    "                'videoId': videoId,\n",
    "                'part': 'snippet',\n",
    "                'order': 'relevance',\n",
    "                'pageToken': pageToken,\n",
    "                'textFormat': 'plainText'\n",
    "            })\n",
    "            assert(resp.ok)\n",
    "\n",
    "            commentData = json.loads(resp.text)\n",
    "\n",
    "            result += [*map(extract_commentThread_text, commentData['items'])]\n",
    "            timestamp += [*map(extract_commentThread_timestamp, commentData['items'])]\n",
    "            likecount += [*map(extract_commentThread_likecount, commentData['items'])]\n",
    "\n",
    "            if not \"nextPageToken\" in commentData:\n",
    "                break\n",
    "            pageToken = commentData[\"nextPageToken\"]\n",
    "\n",
    "            if len(result) >= 1000:\n",
    "                break\n",
    "        except AssertionError:\n",
    "            return [], [], []\n",
    "\n",
    "    return result, timestamp, likecount\n",
    "\n",
    "\n",
    "# 비디오 ID를 받아 자막을 반환한다.\n",
    "def retrieve_captions(videoId: str) -> List[List[str]]:\n",
    "    def extract_caption_text(item):\n",
    "        return re.sub('<(.|\\n)*?>', '', item.text)\n",
    "\n",
    "    resp = requests.get(\n",
    "        f'https://video.google.com/timedtext?lang=en&v={videoId}')\n",
    "    assert(resp.ok)\n",
    "\n",
    "    captionData = ET.fromstring(resp.text)\n",
    "    captions = []\n",
    "    for text in map(extract_caption_text, [*captionData]):\n",
    "        if len(captions) == 0 or captions[-1] != text:\n",
    "            captions.append(text)\n",
    "\n",
    "    return captions\n",
    "\n",
    "\n",
    "# 채널 ID를 받아 최근 비디오 50개의 의 정보를 반환한다. 반환형은 API 참조.\n",
    "def get_recent_videos(channelId: str) -> List[Any]:\n",
    "    resp = retrieve_api('search', params={\n",
    "        'part': 'snippet',\n",
    "        'channelId': channelId,\n",
    "        'type': 'video',\n",
    "        'maxResults': 50,\n",
    "        'order': 'date'\n",
    "    })\n",
    "\n",
    "    return json.loads(resp.text)\n",
    "\n",
    "\n",
    "# 플레이리스트 ID를 받아 앞 50개 비디오의 ID의 리스트를 반환한다.\n",
    "def retrieve_playlist_videos(playlistId: str, cycles: int = 1) -> List[str]:\n",
    "    def extract_playlistItems_videoId(item):\n",
    "        return item['snippet']['resourceId']['videoId']\n",
    "\n",
    "    pageToken = None\n",
    "    result = []\n",
    "\n",
    "    for _ in range(cycles):\n",
    "        resp = retrieve_api('playlistItems', params={\n",
    "            'part': 'snippet',\n",
    "            'maxResults': 50,\n",
    "            'playlistId': playlistId,\n",
    "            'pageToken': pageToken\n",
    "        })\n",
    "        assert(resp.ok)\n",
    "\n",
    "        videoData = resp.json()\n",
    "\n",
    "        if not \"nextPageToken\" in videoData:\n",
    "            break\n",
    "        pageToken = videoData[\"nextPageToken\"]\n",
    "\n",
    "        result += [*map(extract_playlistItems_videoId, videoData['items'])]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 제시된 문자열을 전처리하여 의미 있는 단어의 리스트를 반환한다.\n",
    "def get_words(st: str) -> List[str]:\n",
    "    ALLOWED_POS: Final[str] = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    st = re.sub('[^a-zA-Z\\ ]', ' ', st)  # 공백, a-z, A-Z만 남김\n",
    "    result = word_tokenize(st)  # 토큰화\n",
    "    result = [word for word, pos in filter(\n",
    "        lambda tup: tup[1] in ALLOWED_POS, pos_tag(result))]  # 명사만 추출\n",
    "    result = [*filter(lambda x: x not in stop_words, result)]  # stop words 제거\n",
    "    result = [*map(lambda x: stemmer.stem(x.lower()), result)]  # 소문자화 및 어간 추출\n",
    "    result = [*filter(lambda x: len(x) > 2, result)]  # 최종 결과에서 2글자 이하 단어 제거\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 간단한 multiset 구현\n",
    "def count_at_dict(dt: Dict[str, int], vl: str) -> None:\n",
    "    if vl in dt:\n",
    "        dt[vl] += 1\n",
    "    else:\n",
    "        dt[vl] = 1\n",
    "\n",
    "\n",
    "# 문자열의 리스트를 받아 빈도수 데이터프레임을 반환한다.\n",
    "def get_freq(strList: List[str]) -> pd.DataFrame:\n",
    "    # 모든 단어에 빈도수 저장\n",
    "    dt = dict()\n",
    "    for tokenized in map(get_words, strList):\n",
    "        for word in tokenized:\n",
    "            count_at_dict(dt, word)\n",
    "\n",
    "    # pandas.DataFrame으로 변환\n",
    "    word = []\n",
    "    freq = []\n",
    "    for key in dt:\n",
    "        word.append(key)\n",
    "        freq.append(dt[key])\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        'word': word,\n",
    "        'freq': freq\n",
    "    })\n",
    "\n",
    "    # 빈도수 내림차순으로 정렬\n",
    "    return df.sort_values(by='freq', ascending=False)\n",
    "\n",
    "\n",
    "# 빈도수 데이터프레임을 받아 긍정적, 부정적 단어의 빈도의 튜플을 반환한다.\n",
    "def get_posneg_freq(df: pd.DataFrame, positiveWords: List[str], negativeWords: List[str]) -> Tuple[int, int]:\n",
    "    positiveCount = 0\n",
    "    positiveIndex = 0\n",
    "    positiveLength = len(positiveWords)\n",
    "\n",
    "    negativeCount = 0\n",
    "    negativeIndex = 0\n",
    "    negativeLength = len(negativeWords)\n",
    "\n",
    "    for _, rowSeries in df.sort_values(by='word').iterrows():\n",
    "        if positiveIndex == positiveLength and negativeIndex == negativeLength:\n",
    "            break\n",
    "\n",
    "        while positiveIndex < positiveLength and positiveWords[positiveIndex] < rowSeries['word']:\n",
    "            positiveIndex += 1\n",
    "        if positiveIndex < positiveLength and positiveWords[positiveIndex] == rowSeries['word']:\n",
    "            positiveCount += rowSeries['freq']\n",
    "\n",
    "        while negativeIndex < negativeLength and negativeWords[negativeIndex] < rowSeries['word']:\n",
    "            negativeIndex += 1\n",
    "        if negativeIndex < negativeLength and negativeWords[negativeIndex] == rowSeries['word']:\n",
    "            negativeCount += rowSeries['freq']\n",
    "\n",
    "    return (positiveCount, negativeCount)\n",
    "\n",
    "\n",
    "# 빈도수 데이터프레임을 받아 Word Cloud를 만들고, 저장한다.\n",
    "def draw_wordcloud(df: pd.DataFrame):\n",
    "    wordcloud = WordCloud(background_color='white', width=960, height=540, max_font_size=150).generate_from_frequencies(\n",
    "        {row[1]['word']: row[1]['freq'] for row in df[:100].iterrows()})\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.savefig('wordcloud.svg')\n",
    "\n",
    "\n",
    "# word-time 데이터프레임을 받아 시간별 빈도수 벡터를 반환한다.\n",
    "def get_freq_vec(df: pd.DataFrame, w: str, bins: List[float]) -> List[float]:\n",
    "    se = df[df['word'] == w]['time']\n",
    "    result = np.zeros(len(bins)-1)\n",
    "    for i in range(len(bins)-1):\n",
    "        result[i] = len(se[(bins[i] <= se) & (se < bins[i+1])])\n",
    "\n",
    "    return result / np.linalg.norm(result)\n",
    "\n",
    "\n",
    "# 각 문서별 빈도수 데이터프레임의 리스트를 받아 tf-idf를 계산한다.\n",
    "def tf_idf(word: str, freqList: List[pd.DataFrame]) -> np.ndarray:\n",
    "    # tf: word의 index번째 영상에서의 빈도수 계산\n",
    "    def tf(word: str, index: int) -> int:\n",
    "        filtered = [*freqList[index][freqList[index]['word'] == word]['freq']]\n",
    "\n",
    "        if not filtered:\n",
    "            return 0\n",
    "        else:\n",
    "            return filtered[0]\n",
    "\n",
    "    # idf: word가 출현한 영상의 수 계산\n",
    "    def idf(word: str) -> float:\n",
    "        f = sum([(1 if word in eldf['word'] else 0) for eldf in freqList])\n",
    "        return np.log(len(freqList) / (f + 1))\n",
    "\n",
    "    # tf-idf 계산\n",
    "    return np.array([tf(word, index) for index in range(len(freqList))]) * idf(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driverpath = Path.home() / 'webdriver' / 'chromedriver'\n",
    "driverpath = driverpath.resolve()\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(executable_path=driverpath.as_posix(), options=options)\n",
    "\n",
    "timeout = 3\n",
    "driver.implicitly_wait(timeout)\n",
    "Wait = WebDriverWait(driver, timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_height():\n",
    "    return driver.execute_script('return document.documentElement.scrollHeight')\n",
    "\n",
    "class height_change(object):\n",
    "    def __init__(self):\n",
    "        self.last_height = None\n",
    "\n",
    "    def __call__(self, driver):\n",
    "        page_height = get_page_height()\n",
    "\n",
    "        if self.last_height == None or self.last_height == page_height:\n",
    "            self.last_height = page_height\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "def scroll_all():\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    try:\n",
    "        Wait.until(height_change())\n",
    "    except TimeoutException:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def retrieve_shorts_from_youtube(keyword):\n",
    "    titles = []\n",
    "    videoIds = []\n",
    "\n",
    "    url = f\"https://www.youtube.com/results?search_query=%23shorts+{requests.utils.quote(keyword)}\"\n",
    "    driver.get(url)\n",
    "    Wait.until(EC.url_to_be(url))\n",
    "\n",
    "    while True:\n",
    "        completed = scroll_all()\n",
    "        if completed:\n",
    "            break\n",
    "        \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.select('a#video-title')\n",
    "\n",
    "    for link in links:\n",
    "        title = link.text.replace('\\n', '')\n",
    "        videoId = link.get('href')[9:]\n",
    "\n",
    "        if '#shorts' in title:\n",
    "            titles.append(title)\n",
    "            videoIds.append(videoId)\n",
    "    \n",
    "    return titles, videoIds\n",
    "\n",
    "def retrieve_comments_selenium(videoId):\n",
    "    url = f\"https://www.youtube.com/watch?v={videoId}\"\n",
    "    driver.get(url)\n",
    "    Wait.until(EC.url_to_be(url))\n",
    "\n",
    "    while True:\n",
    "        completed = scroll_all()\n",
    "        if completed:\n",
    "            break\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    commentElems = soup.select('yt-formatted-string#content-text')\n",
    "    return [*map(lambda elem: str(elem.text), commentElems)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(directoryName: str, statistic: bool = False, timestamp: bool = False, likecount: bool = False):\n",
    "    with open(f'{directoryName}/videoid.txt', 'w') as f:\n",
    "        f.write(str(videoIdList))\n",
    "    with open(f'{directoryName}/comments.txt', 'w') as f:\n",
    "        f.write(str(commentsList))\n",
    "    if statistic:\n",
    "        with open(f'{directoryName}/statistics.txt', 'w') as f:\n",
    "            f.write(str(statisticsList))\n",
    "    if timestamp:\n",
    "        with open(f'{directoryName}/timestamps.txt', 'w') as f:\n",
    "            f.write(str(timestampsList))\n",
    "    if likecount:\n",
    "        with open(f'{directoryName}/likecounts.txt', 'w') as f:\n",
    "            f.write(str(likecountsList))\n",
    "\n",
    "def load_data(directoryName: str, statistic: bool = False, timestamp: bool = False, likecount: bool = False):\n",
    "    global videoIdList, commentsList, timestampsList\n",
    "\n",
    "    with open(f'{directoryName}/videoid.txt', 'r') as f:\n",
    "        videoIdList = eval(f.read())\n",
    "    with open(f'{directoryName}/comments.txt', 'r') as f:\n",
    "        commentsList = eval(f.read())\n",
    "    if statistic:\n",
    "        global statisticsList\n",
    "        with open(f'{directoryName}/statistics.txt', 'r') as f:\n",
    "            statisticsList = eval(f.read())\n",
    "    if timestamp:\n",
    "        global timestampsList\n",
    "        with open(f'{directoryName}/timestamps.txt', 'r') as f:\n",
    "            timestampsList = eval(f.read())\n",
    "    if likecount:\n",
    "        global likecountsList\n",
    "        with open(f'{directoryName}/likecounts.txt', 'r') as f:\n",
    "            likecountsList = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, videoIdList = retrieve_shorts_from_youtube('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 425/425 [16:22<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "commentsList = []\n",
    "timestampsList = []\n",
    "likecountsList = []\n",
    "statisticsList = []\n",
    "for videoId in tqdm(videoIdList):\n",
    "    statisticsList.append(retrieve_statistics(videoId))\n",
    "\n",
    "    comments, timestamps, likecounts = retrieve_comments(videoId)\n",
    "    commentsList.append(comments)\n",
    "    timestampsList.append(timestamps)\n",
    "    likecountsList.append(likecounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_shorts_1013\n",
    "save_data(\"data_shorts_1013\", timestamp=True, statistic=True, likecount=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d992dc99bef6935113868216bda8f8f78b2ccfafefe3a81c650196168d5e8fa4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
