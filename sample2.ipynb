{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0ba7409515b3693502c21990171afc5cef8184142f09e8cdd61aaea9ca6ef25ac",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import copy\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyB2OJynYIkD7nW7ymSGtmkSHp9iMVN1K-M' # API 요청을 위한 키\n",
    "BASE_URL = 'https://www.googleapis.com/youtube/v3/' # url prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api 엔드포인트에 요청을 보내는 헬퍼 함수\n",
    "def retrieve_api(url, params={}, data={}, headers={}, method='GET'):\n",
    "    params = copy.deepcopy(params)\n",
    "    params.update({\n",
    "        'key': API_KEY\n",
    "    })\n",
    "    \n",
    "    return requests.request(method, urllib.parse.urljoin(BASE_URL, url), params=params, data=data, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_commentThread_text = lambda item: item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "\n",
    "def get_comments(videoId):\n",
    "    pageToken = None\n",
    "    result = []\n",
    "\n",
    "    while True:\n",
    "        resp = retrieve_api('commentThreads', params={\n",
    "            'videoId': videoId,\n",
    "            'part': 'snippet',\n",
    "            'order': 'relevance',\n",
    "            'pageToken': pageToken,\n",
    "            'textFormat': 'plainText'\n",
    "        })\n",
    "        assert(resp.ok)\n",
    "        \n",
    "        commentData = json.loads(resp.text)\n",
    "\n",
    "        if not \"nextPageToken\" in commentData:\n",
    "            break\n",
    "        pageToken = commentData[\"nextPageToken\"]\n",
    "        \n",
    "        result += [*map(extract_commentThread_text, commentData['items'])]\n",
    "\n",
    "        print(len(result), end=' ')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_caption_text = lambda item: re.sub('<(.|\\n)*?>', '', item.text)\n",
    "\n",
    "def get_captions(videoId):\n",
    "    resp = requests.get(f'https://video.google.com/timedtext?lang=en&v={VIDEO_ID}')\n",
    "    assert(resp.ok)\n",
    "\n",
    "    captionData = ET.fromstring(resp.text)\n",
    "    captions = []\n",
    "    for text in map(extract_caption_text, [*captionData]):\n",
    "        if len(captions) == 0 or captions[-1] != text:\n",
    "            captions.append(text)\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hamerin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/hamerin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/hamerin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(st: str):\n",
    "    st = re.sub('[^a-zA-Z\\ ]', '', st) # 공백, a-z, A-Z만 남딤\n",
    "    result = word_tokenize(st.lower()) # 소문자로 바꾸고, 토큰화\n",
    "    result = [*filter(lambda x: x not in stop_words, result)] # stop words 제거\n",
    "    result = [*map(lambda x: lemmatizer.lemmatize(x), result)] # 표제어 추출\n",
    "\n",
    "    return result\n",
    "\n",
    "# 간단한 multiset 구현\n",
    "def counter(dt, vl):\n",
    "    if vl in dt:\n",
    "        dt[vl] += 1\n",
    "    else:\n",
    "        dt[vl] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(data):\n",
    "    # 모든 단어에 빈도수 저장\n",
    "    dt = dict()\n",
    "    for tokenized in map(get_words, data):\n",
    "        for word in tokenized:\n",
    "            counter(dt, word)\n",
    "\n",
    "    # pandas.DataFrame으로 변환\n",
    "    word = []\n",
    "    freq = []\n",
    "    for key in dt:\n",
    "        word.append(key)\n",
    "        freq.append(dt[key])\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        'word': word,\n",
    "        'freq': freq\n",
    "    })\n",
    "\n",
    "    # 빈도수 내림차순으로 정렬\n",
    "    return df.sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20 40 60 79 99 119 139 159 179 199 219 239 259 279 298 317 337 357 377 397 417 437 "
     ]
    }
   ],
   "source": [
    "comments = get_comments('y-7UG0jORoA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-59-4067729c525e>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-4067729c525e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    # display(get_freq(comments))\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    # display(get_freq(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveWords = pd.read_csv('positive.txt')['words']\n",
    "negativeWords = pd.read_csv('negative.txt')['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0          2-faced\n",
       "1          2-faces\n",
       "2         abnormal\n",
       "3          abolish\n",
       "4       abominable\n",
       "           ...    \n",
       "4778          zaps\n",
       "4779        zealot\n",
       "4780       zealous\n",
       "4781     zealously\n",
       "4782        zombie\n",
       "Name: words, Length: 4783, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pn_freq(df: pd.DataFrame):\n",
    "    df = df.sort_values(by='word')\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1870\n"
     ]
    }
   ],
   "source": [
    "get_pn_freq(get_freq(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}